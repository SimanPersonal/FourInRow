<!DOCTYPE html>
<html>
<head>
    <title>Whistle to Piano</title>
    <style>
        .controls {
            margin: 20px;
        }
        button {
            padding: 10px;
            margin: 5px;
        }
        #status {
            margin: 20px;
            font-family: monospace;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/tone@14.7.77/build/Tone.min.js"></script>
</head>
<body>
    <div class="controls">
        <button onclick="startRecording()">Start Recording</button>
        <button onclick="stopRecording()">Stop Recording</button>
        <button onclick="playback()">Play as Piano</button>
    </div>
    <div id="status">Ready to record...</div>

    <script>
        let audioContext;
        let mediaRecorder;
        let audioChunks = [];
        let analyser;
        let recordedFrequencies = [];
        const piano = new Tone.Sampler({
            urls: {
                A0: "A0.mp3",
                C1: "C1.mp3",
                "D#1": "Ds1.mp3",
                "F#1": "Fs1.mp3",
                A1: "A1.mp3",
                C2: "C2.mp3",
                "D#2": "Ds2.mp3",
                "F#2": "Fs2.mp3",
                A2: "A2.mp3",
                C3: "C3.mp3",
                "D#3": "Ds3.mp3",
                "F#3": "Fs3.mp3",
                A3: "A3.mp3",
                C4: "C4.mp3",
                "D#4": "Ds4.mp3",
                "F#4": "Fs4.mp3",
                A4: "A4.mp3",
                C5: "C5.mp3",
                "D#5": "Ds5.mp3",
                "F#5": "Fs5.mp3",
                A5: "A5.mp3",
                C6: "C6.mp3",
                "D#6": "Ds6.mp3",
                "F#6": "Fs6.mp3",
                A6: "A6.mp3",
                C7: "C7.mp3",
                "D#7": "Ds7.mp3",
                "F#7": "Fs7.mp3",
                A7: "A7.mp3",
                C8: "C8.mp3"
            },
            baseUrl: "https://tonejs.github.io/audio/salamander/",
        }).toDestination();
        
        async function startRecording() {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                
                const source = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048;
                source.connect(analyser);
                
                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };
                
                mediaRecorder.start(100);
                document.getElementById('status').textContent = 'Recording...';
                
                // Start frequency detection
                detectPitch();
                
            } catch (err) {
                console.error('Error accessing microphone:', err);
                document.getElementById('status').textContent = 'Error: ' + err.message;
            }
        }

        function detectPitch() {
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Float32Array(bufferLength);
            
            function analyze() {
                if (mediaRecorder.state === 'recording') {
                    analyser.getFloatFrequencyData(dataArray);
                    
                    // Find the peak frequency
                    let maxValue = -Infinity;
                    let maxIndex = 0;
                    
                    for (let i = 0; i < bufferLength; i++) {
                        if (dataArray[i] > maxValue) {
                            maxValue = dataArray[i];
                            maxIndex = i;
                        }
                    }
                    
                    const frequency = maxIndex * audioContext.sampleRate / analyser.fftSize;
                    if (frequency > 100 && maxValue > -50) { // Filter out noise
                        recordedFrequencies.push(frequency);
                    }
                    
                    requestAnimationFrame(analyze);
                }
            }
            
            analyze();
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                document.getElementById('status').textContent = 'Recording stopped';
            }
        }

        function frequencyToNote(frequency) {
            const A4 = 440;
            const notes = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"];
            const halfStepsFromA4 = Math.round(12 * Math.log2(frequency / A4));
            const octave = Math.floor((halfStepsFromA4 + 57) / 12); // 57 is the number of semitones from C0 to A4
            const noteIndex = (halfStepsFromA4 + 57) % 12;
            return `${notes[noteIndex]}${octave}`;
        }

        async function playback() {
            if (recordedFrequencies.length === 0) {
                document.getElementById('status').textContent = 'No recording to play';
                return;
            }

            await Tone.start();
            document.getElementById('status').textContent = 'Playing back...';
            
            const now = Tone.now();
            recordedFrequencies.forEach((freq, index) => {
                const note = frequencyToNote(freq);
                piano.triggerAttackRelease(note, "8n", now + index * 0.25);
            });
        }
    </script>
</body>
</html>
